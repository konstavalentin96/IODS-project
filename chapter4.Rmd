# Week 4 - Clustering and classification

## Analysis

```{r}
# Load the Boston data set from the MASS package, along with the libraries needed in the analysis
library(MASS)
data("Boston")
library(ggplot2)
library(GGally)
library(corrplot)
```


### Overview of the data

```{r}
# Explore the structure and dimensions of the Boston data set
str(Boston)
dim(Boston)
```

The Boston data set has 14 variables (all numeric or integers) and 506 observations. It contains information on housing values of Boston.


```{r}
# Summary of the variables
summary(Boston)
```

It seems that most of the variables are continuous while some are categorical or can only take whole numbers as values; 'chas' variable is 1 when the land area bounds a river, otherwise it is 0; 'rad' is an index of accessibility to radial highways which seems to take values (whole numbers) from 1 to 24. 


```{r}
# Plot matrix of the variables
pairs(Boston)
```

```{r}
# Let's look at the correlations in the data:
# calculate the correlation matrix while rounding the correlations to 2 digits
cor_matrix <- cor(Boston) %>% round(digits = 2)

# Visualize the correlation matrix
corrplot(cor_matrix, method="circle")
```


Here you can see the correlations between the variables represented by the size and colour of the circles: blue means positive correlation and red means negative correlation (weird colour choices I must say...) while the size also represents the strength of the correlation. Here we can see that some of the most strong negative correlations are between dis (weighted mean of distances to five Boston employment centres) and age (proportion of owner-occupied units built prior to 1940), nox (nitrogen oxides concentration (parts per 10 million)) and indus (proportion of non-retail business acres per town), while the strongest positive correlation is between tax (full-value property-tax rate per $10,000) and rad (index of accessibility to radial highways). 


### Strandardize the data set

```{r}
# center and standardize variables
boston_scaled <- scale(Boston)

# summaries of the scaled variables
summary(boston_scaled)

```

By scaling the variables we have now the distance of the individual values from the mean of the variable measured by the number of standard deviations; the mean is 0 since the distance from the mean in that case is 0, and in case of the crim variable, the maximum value is roughly 9.92 standard deviations away from the mean. 



```{r}
# Converting the crim variable to categorical:
# change the object to data frame so we can handle it better
boston_scaled <- as.data.frame(boston_scaled)

# create a quantile vector of crim
bins <- quantile(boston_scaled$crim)

# create a categorical variable 'crime'
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, labels = c("low", "med_low", "med_high", "high"))


# remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)

# add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)



# Dividing the data set into train and test sets

# choose randomly 80% of the rows
ind <- sample(nrow(boston_scaled),  size = nrow(boston_scaled) * 0.8)

# create train set
train <- boston_scaled[ind,]

# create test set 
test <- boston_scaled[-ind,]

```




### Fitting a linear discriminant analysis (LDA) on the train data set while using the crime variable as the target and all the other variables as predictors

```{r}
# Create the LDA object using the train set
lda.fit <- lda(crime ~ ., data = train)

# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  graphics::arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$crime)

# plot the lda results (select both lines and execute them at the same time!)
plot(lda.fit, dimen = 2,col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 1)
```


```{r}
# save the correct classes from test data
correct_classes <- test$crime

# remove the crime variable from test data
test <- dplyr::select(test, -crime)

# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

# cross tabulate the results
table(correct = correct_classes, predicted = lda.pred$class)
```


By category, the accuracy of the predictions are:
- Low: 19 / (19+4+1) = 79 %
- Med_low: 15 / (1+15+7) = 65 %
- Med_high: 14 / (11+14+3) = 50 %
- High: 26 / (1+26) = 96 % 

It seems that the model predicts the highest and lowest crime rates the best, while the predictions of med_high rates were just as good as guessing.


```{r}
# Reloading and re-standardizing the data set
data("Boston")
boston_scaled <- scale(Boston)

# Calculating the euclidean distances and creating a distance matrix
dist_eu <- dist(boston_scaled)

# Set seed so you always get the same clusters when running the kmeans function
set.seed(13)

# k-means clustering with four clusters
km <- kmeans(boston_scaled, centers = 4)

# plot the Boston dataset with clusters
pairs(boston_scaled, col = km$cluster)

```

```{r}
# Investigating the optimal number of clusters
# determine the number of clusters; 10 will do initially
k_max <- 10

# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(boston_scaled, k)$tot.withinss})

# visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line')
```

The optimal number of clusters is the point in the graph above where the line drops dramatically; in this case it seems to be at 2 --> optimal number of clusters for this data set is two.

```{r}
# k-means clustering with two clusters
km <- kmeans(boston_scaled, centers = 2)

# plot again
pairs(boston_scaled, col = km$cluster)
```

The two colours in the graph (red and black) represent the two clusters we created; they seem to differ from each other in almost every comparison of the variables (you can see there are visibly separate red and black clusters). The difference here is based on the euclidean distance calculated a few steps above; the more similar two cases are from each other, the shorter the distance is and vice versa. Based on our model, there is a threshold after which cases are categorised in one of the two clusters you can see in the plot. 


