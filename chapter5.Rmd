# Week 5 - Dimensionality reduction techniques

## Analysis

### Preparations and the overview of the data

```{r}
# Load the needed packages
library(tibble)
library(GGally)
library(corrplot)
library(ggplot2)
library(FactoMineR)

```

```{r}
# Load the data
human <- read.csv("C:/Users/kveu/Work Folders/PhD Research/Courses/IODS/IODS-project/Data/human.csv")
```

```{r}
# Move country names to rownames
human <- column_to_rownames(human, "Country")
```

```{r}
# Visualise the data
ggpairs(human)
```


```{r}
# Visualise the relationships between the variables a bit better
cor(human) %>% 
  corrplot()
```


```{r}
# Summaries of the variables
summary(human)
```


As can be seen from the ggpairs-plot, the distributions are quite nicely normally distributed. In the weirder-looking cases such as Ado.Birth (adolescent birth rate) or Mat.Mor (maternal mortality ratio), it is expected to be like that since the scales start at 0 and a large proportion of people have very low scores in these. It seems that several variables are correlated with each other, like Mat.Mor and Life.Exp (life expectancy, negatively correlated) or Ado.Birth and Mat.Mor (positively correlated), as expected. 


### PCA on raw data

```{r}
# Perform principal component analysis using Singular Value Decomposition (SVD)
pca_human <- prcomp(human)

# Save the summary of the model
pca_sum <- summary(pca_human)

# Show the variability captured by the PCs as percentages
var <- round(100*pca_sum$importance[2, ], digits = 1)
var

# create object pc_lab to be used as axis labels
pc_lab <- paste0(names(var), " (", var, "%)")
pc_lab

# draw a biplot
biplot(pca_human, cex = c(0.8, 1), col = c("grey40", "deeppink2"), xlab = var[1], ylab = var[2])
```


### Scaling the data and re-doing the PCA

```{r}
# Scale the data
human_std <- scale(human)

# Perform principal component analysis using Singular Value Decomposition (SVD)
pca_human_ <- prcomp(human_std)

# Save the summary of the model
pca_sum_ <- summary(pca_human_)

# Show the variability captured by the PCs as percentages
var_ <- round(100*pca_sum_$importance[2, ], digits = 1)
var_

# create object pc_lab to be used as axis labels
pc_lab_ <- paste0(names(var_), " (", var_, "%)")
pc_lab_

# draw a biplot
biplot(pca_human_, cex = c(0.8, 1), col = c("grey40", "deeppink2"), xlab = pc_lab_[1], ylab = pc_lab_[2])
```

Without scaling, the variables have different scales and variances and thus cannot be compared to each other in analysis like this; it gives some variables too much or too little weight. This has happened in the PCA done on raw data: all the variance is "explained" by the first PC. In the graph, we can see that GNI is the variable explaining most of the variance and all the other variables are clumped in together in the same spot on the graph. This is because the model has given too much weight to the GNI variable, which has values in the tens of thousands while others have values around the scale from 1-100. 

By standardising, we center the data by subtracting the mean of each variable from each observation of the variable, and we scale the data by dividing those values by the standard deviation of the corresponding variables. This way the scales and variances are all comparable with each other, while retaining the original relative order (the observations with the highest values are still the highest although the scale is different). Principal components are in the order of importance; PC1 explains the most of the variance in the data, PC2 the second most etc. 

Now, in the results of the analysis using the standardised data, we can see that the PC1 explains 53.6 % of the variance in the data, while PC2 explains 16.2%. In the biplot we can see that this is the case; the variables are distributed more on the line with the PC1 axis than the PC2 axis. 

Personal interpretation on PC1 and PC2 based on the second (standardised) plot:

The specific variables can be a bit hard to see from the graph but we can clearly see that there are three clusters of variables; one on the left, one on the right and the one above. The variables in the same "cluster" are positively correlated with each other (there is a small angle between them), variables with around 180 degree angles between them (in this case, variables in the left vs right side clusters) are negatively correlated with each other, and variables with around 90 degree angle between them (in this case, variables in the cluster up in the graph vs the other variables) are not correlated with each other. The variables are also quite well aligned with the PC axes; Labo.FM and Parli.F are aligned with PC2 and the others with PC1, so they are well correlated with those dimensions of the data. PC1 and PC2 explain almost 70 % of the variance in the data, and it can be seen here that it matches well. 


### Tea time

```{r}
# Load the tea data 
tea <- read.csv("https://raw.githubusercontent.com/KimmoVehkalahti/Helsinki-Open-Data-Science/master/datasets/tea.csv", stringsAsFactors = TRUE)
```


```{r}
# Explore data
str(tea)
dim(tea)
View(tea)
```


```{r}
# Keep only certain columns
library(dplyr)
library(tidyr)
tea <- subset(tea, select = c("Tea", "How", "how", "sugar", "where", "lunch")) # For some reason the index file didn't knit when using 'select' function so had to use 'subset'

# Visualise the tea data
pivot_longer(tea, cols = everything()) %>% 
  ggplot(aes(value)) + facet_wrap("name", scales = "free") +
  geom_bar() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 8))
```

```{r}
# Perform MCA
mca <- MCA(tea, graph = FALSE)

# summary of the model
summary(mca)

# visualize MCA
plot(mca, invisible=c("ind"), graph.type = "classic", habillage = "quali")

```


MCA can be viewed as PCA for qualitative data. In this case, it is used to analyse data from a survey about tea consumption.In the mca results table, we can see the categories and their statistical test (v.test); if the test is above 1.96 or less than -1.96, the categorie significantly contributes to the corresponding dimension. From the results we can see that for example the categorie unpackaged contributes the most negatively to the dimension 1 (ctr = -12.023) (as can be seen from the graph, tea shop would contribute the most but only 10 categories are shown in the summary).

In the graph above we can see the labels of the variables we choose in the analysis, colour representing the variable the labels describe. The dimensions can be viewed as the PCs in principal component analysis; they try to capture as much variability of the data as possible. In this case, dimension 1 captures 15.2 % and dimension 2 14.2 % of the variance in the data, which is not much and so only showing the two dimensions at a time may not be representative of the data. 

